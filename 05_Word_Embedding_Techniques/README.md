# ğŸ”  **Word Embedding Techniques using Keras**

## ğŸ“˜ **Project Description**
...>This project demonstrates **Word Embedding Techniques** using the **Keras Embedding Layer**.  
...>It covers steps from **One-Hot Encoding â†’ Padding â†’ Word Embedding Representation**,  showing how words are converted into dense numerical vectors that capture meaning.

---

## ğŸš€ **Steps Covered**
1ï¸âƒ£ **Sentence Tokenization** â€” Converting sentences into words  
2ï¸âƒ£ **One-Hot Encoding** â€” Assigning unique numbers to words  
3ï¸âƒ£ **Padding Sequences** â€” Making all sentences of equal length  
4ï¸âƒ£ **Word Embedding Layer** â€” Mapping words to dense 10-dimensional vectors  
5ï¸âƒ£ **Prediction & Output** â€” Generating vector representation for each word  

---

## ğŸ§  **Key Parameters**
- ğŸ§© `voc_size = 500` â†’ Total unique words  
- ğŸ“ `sent_length = 8` â†’ Fixed input length after padding  
- ğŸ“Š `embedding_dim = 10` â†’ Size of each word vector  
- âš™ï¸ `optimizer = 'adam'` and `loss = 'mse'`

---

## ğŸ“ˆ **Sample Output**
```
Layer (type)           Output Shape        Param #
-------------------------------------------------
embedding (Embedding)  (None, 8, 10)       5,000
-------------------------------------------------
Total params: 5,000 (19.53 KB)
Trainable params: 5,000 (19.53 KB)
Non-trainable params: 0 (0.00 B)
```

---

## ğŸ¯ **Result**
Each word is represented as a dense 10-dimensional vector,  
allowing the model to understand **semantic relationships** between words.

---

## ğŸ§° **Libraries Used**
- ğŸ Python  
- ğŸ’« TensorFlow / Keras  
- ğŸ”¢ NumPy  

---

## âœ¨ **Author**
ğŸ‘©â€ğŸ’» *Akanksha Mishra*  
â€œExploring NLP step-by-step â€” from tokens to meaning!â€

