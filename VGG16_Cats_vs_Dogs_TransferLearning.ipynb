{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGmYBVypWvMmYIFMqxHMx7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akanksha131313/AI_Projects_Journey/blob/main/VGG16_Cats_vs_Dogs_TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataset Explanation (Short):**\n",
        "\n",
        "The dataset contains images of cats and dogs.\n",
        "Each image belongs to one of two categories — \"cats\" or \"dogs\".\n",
        "We’ll use ImageDataGenerator for data augmentation and resizing all images to 150×150 pixels — which is the required input size for VGG16"
      ],
      "metadata": {
        "id": "W6wZ_is0HkWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1) Utilize VGG16 for Feature Extraction-**"
      ],
      "metadata": {
        "id": "ZGUbJUazHvMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**a) Employ the VGG16 model, excluding its top layers, to serve as a feature extractor for cat and dog images-**"
      ],
      "metadata": {
        "id": "ne6xCkT6I-WF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Load Cats vs Dogs dataset\n",
        "(raw_train, raw_test), metadata = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split=['train[:80%]', 'train[80%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True\n",
        ")\n",
        "\n",
        "# Set image size as required by VGG16\n",
        "IMG_SIZE = 150\n",
        "\n",
        "# Resize and normalize images (0–1 range)\n",
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "train = raw_train.map(format_image)\n",
        "test = raw_test.map(format_image)\n",
        "\n",
        "# Batch data for training\n",
        "BATCH_SIZE = 32\n",
        "train_batches = train.shuffle(1000).batch(BATCH_SIZE)\n",
        "test_batches = test.batch(BATCH_SIZE)\n",
        "\n",
        "# Load pre-trained VGG16 (without top layers)\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
        "\n",
        "# Freeze convolutional layers to retain pre-trained features\n",
        "for layer in vgg_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "print(\"VGG16 base model loaded successfully as feature extractor.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRzKv4q5cPUm",
        "outputId": "79c214ee-ad2f-4321-9238-5dfb2a862fff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG16 base model loaded successfully as feature extractor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**b) Ensure the input images are of the correct size (150x150) and preprocessed appropriately to match VGG16’s requirements-**"
      ],
      "metadata": {
        "id": "7RIRL28sKdNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify image shape and range\n",
        "for image, label in train.take(1):\n",
        "    print(\"Image shape:\", image.shape)\n",
        "    print(\"Pixel range:\", tf.reduce_min(image).numpy(), \"to\", tf.reduce_max(image).numpy())\n",
        "\n",
        "# Confirm batches created\n",
        "print(\"Train batches:\", train_batches)\n",
        "print(\"Test batches:\", test_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJQyEIU5cSD9",
        "outputId": "51d5ba4e-bba0-49bd-ed50-e5080c23dfa9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (150, 150, 3)\n",
            "Pixel range: 0.0 to 1.0\n",
            "Train batches: <_BatchDataset element_spec=(TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "Test batches: <_BatchDataset element_spec=(TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2) Data Preprocessing and Augmentation-**"
      ],
      "metadata": {
        "id": "m3HnJWGEcYM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Implement image data generators for real-time augmentation-**"
      ],
      "metadata": {
        "id": "J5JdTio8cevZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Apply augmentation on-the-fly (rotation, shift, flip)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Note: Here we already used tfds dataset, so datagen is for demonstration\n",
        "print(\"Data augmentation setup complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfvLBVL9cpSS",
        "outputId": "07ae6ac9-28bd-44c3-f55e-6405114a0887"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data augmentation setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3) Build and Train the Classification Model-**"
      ],
      "metadata": {
        "id": "rO8npbF1c8Bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Add custom fully connected layers on top of the VGG16 model for classification-**"
      ],
      "metadata": {
        "id": "AEKcwIgLc_gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Add new layers on top of VGG16\n",
        "model = Sequential([\n",
        "    vgg_base,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "print(\"Custom classification layers added.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvXIVM5ZdM9B",
        "outputId": "26f9f740-5ff6-44e8-b769-596a53c56bb7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom classification layers added.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Freeze the convolutional layers of VGG16 and train only the custom layers-**"
      ],
      "metadata": {
        "id": "k1w5tHfXdheo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=SGD(learning_rate=0.001, momentum=0.9),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_batches,\n",
        "    epochs=5,\n",
        "    validation_data=test_batches\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa61xDwcdnPn",
        "outputId": "85625a7a-477b-4420-e7a0-b1d740ed08a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 155ms/step - accuracy: 0.7708 - loss: 0.4574 - val_accuracy: 0.8917 - val_loss: 0.2623\n",
            "Epoch 2/5\n",
            "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 135ms/step - accuracy: 0.8778 - loss: 0.2878 - val_accuracy: 0.9043 - val_loss: 0.2367\n",
            "Epoch 3/5\n",
            "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 136ms/step - accuracy: 0.8854 - loss: 0.2631 - val_accuracy: 0.9058 - val_loss: 0.2295\n",
            "Epoch 4/5\n",
            "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 135ms/step - accuracy: 0.8926 - loss: 0.2490 - val_accuracy: 0.9091 - val_loss: 0.2156\n",
            "Epoch 5/5\n",
            "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 135ms/step - accuracy: 0.8961 - loss: 0.2341 - val_accuracy: 0.9093 - val_loss: 0.2173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4) Evaluate and Test the Model-**"
      ],
      "metadata": {
        "id": "zUtwN_3sdvqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = model.evaluate(test_batches)\n",
        "print(f\"Validation Accuracy: {val_acc*100:.2f}%  |  Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-Ls-ML6d3HI",
        "outputId": "769763c9-b4b0-4da1-cd78-525a21c6c75a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 106ms/step - accuracy: 0.9137 - loss: 0.2056\n",
            "Validation Accuracy: 90.93%  |  Validation Loss: 0.2173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5) Save and Reload the Model-**"
      ],
      "metadata": {
        "id": "NqegtGcnfol5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model.save(\"vgg16_cats_dogs.keras\")\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# Reload and verify\n",
        "loaded_model = load_model(\"vgg16_cats_dogs.keras\")\n",
        "loss, acc = loaded_model.evaluate(test_batches)\n",
        "print(f\"Loaded Model Accuracy: {acc*100:.2f}%  |  Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWGGqyGWfuMJ",
        "outputId": "83eb1389-23f1-45d3-9162-09ed44bd6511"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n",
            "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 116ms/step - accuracy: 0.9137 - loss: 0.2056\n",
            "Loaded Model Accuracy: 90.93%  |  Loss: 0.2173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Conclusion-**\n",
        "\n",
        "\n",
        "The transfer learning model using VGG16 successfully classified cat and dog images.\n",
        "It achieved around 91% accuracy with minimal loss (~0.21), showing strong generalization.\n",
        "Pre-trained convolutional layers effectively extracted image features,\n",
        "while the custom dense layers performed accurate classification.\n",
        "The saved model was reloaded successfully, maintaining consistent performance.\n"
      ],
      "metadata": {
        "id": "dUTlqBcDgATc"
      }
    }
  ]
}