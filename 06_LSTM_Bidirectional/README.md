# ğŸ“° Fake News Detection using Bidirectional LSTM ğŸš€  

## ğŸ” Overview  
This project builds a **Fake News Detection Model** using a **Bidirectional LSTM (BiLSTM)** network â€” enabling the model to learn from both **past and future word context** for improved accuracy.  
With deep text understanding, the model achieved an impressive **ğŸ”¥ 86% accuracy** on the dataset.  

---

## ğŸ§  Key Highlights  
- ğŸ§¹ **Data Cleaning:** Handled NaN values and removed invalid label entries.  
- ğŸ”¤ **Text Preprocessing:** Tokenization, stopword removal, stemming & padding for uniform input.  
- ğŸ’¬ **Embedding Layer:** Converts text into dense vectors before feeding into BiLSTM.  
- ğŸ” **Model Architecture:**  
  - Embedding Layer  
  - **Bidirectional LSTM (128 units)** for dual-context learning  
  - Dense + Sigmoid output for binary classification  
- ğŸ“Š **Evaluation Metrics:** Accuracy, Confusion Matrix, and Classification Report  

---

## âš™ï¸ Tech Stack  
- ğŸ **Python**  
- ğŸ§  **TensorFlow / Keras**  
- ğŸ“ˆ **Scikit-learn**  
- ğŸ—‚ï¸ **Pandas**, **NumPy**, **NLTK**

---

## ğŸ“¦ Dataset  
- ğŸ“° **Dataset Size:** ~71K news samples (title, text, label)  
- ğŸ”§ Cleaned to remove missing and noisy data  
- ğŸ¯ Labels: 1 â†’ Fake | 0 â†’ Real  

---

## ğŸ“Š Results  
| Metric | Value |
|:-------:|:------:|
| ğŸ§  Model | **Bidirectional LSTM** |
| ğŸ¯ Accuracy | **86%** |
| âš¡ Loss | Decreased steadily without NaN issues |
| ğŸ’¡ Insight | Capturing bidirectional context significantly boosted performance |

---

## ğŸŒŸ Future Enhancements  
- ğŸ§© Add **Attention Mechanism** for deeper interpretability  
- ğŸ’¡ Try **Pretrained Embeddings (GloVe / Word2Vec)**  
- ğŸ” Experiment with **Stacked BiLSTM** layers  
- ğŸ¤– Explore **Transformer-based models (BERT, RoBERTa)**  

---

## ğŸ’» How to Run  
```bash
pip install tensorflow pandas numpy scikit-learn nltk
python fake_news_bilstm.py
